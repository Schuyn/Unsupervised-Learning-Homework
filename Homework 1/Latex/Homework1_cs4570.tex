\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{pythonhighlight}
\usepackage{subcaption} % 并列放图时可以分别打标签


% 页边距
\geometry{margin=1in}

% 页眉页脚
\pagestyle{fancy}
\fancyhf{}
\lhead{STAT 5244 -- Unsupervised Learning}
\rhead{HW1}
\cfoot{\thepage\ of \pageref{LastPage}}

% -------------------------------
% 数学环境
% -------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

% 向量/矩阵粗体
\renewcommand{\vec}[1]{\bm{#1}}

% -------------------------------
% 代码高亮
% -------------------------------
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  frame=single,
  breaklines=true,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  tabsize=4
}

% -------------------------------
% 文档开始
% -------------------------------
\begin{document}

\begin{center}
    {\Large \textbf{STAT 5244 -- Unsupervised Learning}}\\[6pt]
    \textbf{Homework 1}\\[6pt]
    Name: \underline{Chuyang Su} \quad UNI: \underline{cs4570}
\end{center}

\hrule
\vspace{1em}

% -------------------------------
% Problem 1
% -------------------------------
\section{Dimension Reduction on Digits Data.}
%-------------------------------
% Problem 1a
% ------------------------------
\subsection{Apply linear dimension reduction techniques.}
In this experiment, I applied three linear dimension reduction methodsand compared their performance on the \texttt{scikit-learn} \textit{Digits} dataset ($n = 1797$, $p = 64$).  

Each method projects the data into a two-dimensional latent space, on which I visualized the results and quantitatively evaluated their ability to separate the ten digit classes.

The results of this experiment are summarized below.

\begin{figure}[h!]
    \centering
    % --- PCA ---
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Results/Problem_1_a/pca_2d.png}
        \caption{PCA}
        \label{fig:pca-2d}
    \end{subfigure}
    % --- NMF ---
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Results/Problem_1_a/nmf_2d.png}
        \caption{NMF$\rightarrow$PCA}
        \label{fig:nmf-2d}
    \end{subfigure}
    % --- ICA ---
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Results/Problem_1_a/ica_2d.png}
        \caption{ICA}
        \label{fig:ica-2d}
    \end{subfigure}

    \caption{2D embeddings of the digits data using PCA, NMF$\rightarrow$PCA, and ICA. Colors denote true digit labels.}
    \label{fig:linear-2d-all}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Method} & \textbf{ARI} & \textbf{NMI} & \textbf{Silhouette}\\
    \midrule
    PCA & \textbf{0.3614} & \textbf{0.5190} & 0.3993 \\
    NMF $\rightarrow$ PCA & 0.1588 & 0.2961 & \textbf{0.4080} \\
    ICA & 0.3175 & 0.4567 & 0.3673 \\
    \bottomrule
    \end{tabular}
    \caption{Quantitative comparison of linear dimension reduction methods. The best scores for each metric are bolded.}
\end{table}

%-------------------------------
% PCA
% ------------------------------
\paragraph{PCA.}
For PCA, I retained the first two principal components and projected the samples into the 2D subspace they span, which preserves the primary directions of variance. 
As for hyperparameters, PCA has very few tunable parameters---the main one being the number of principal components.  
For ease of visualization, I set the number of components to $2$ (PC=2) in this experiment.  
Figure~\ref{fig:pca-2d} shows that the data are well dispersed, and digits such as 0, 3, 4, 6, and 9 form clearly separated clusters. Quantitatively, PCA achieved an ARI of 0.3614, an NMI of 0.5190, and a Silhouette score of 0.3993. 
Except for the Silhouette score, PCA obtained the highest values among the three methods.  
This indicates that PCA effectively separates the digits and maintains a high level of consistency with the true labels.  
Although its Silhouette value (approximately 0.4) is not the highest, it still suggests reasonably compact and well-separated clusters.  
This minor difference can be attributed to slight overlaps between neighboring clusters in the 2D embedding, even though the overall structure aligns well with the ground-truth classes.

In addition to the 2D embedding, I plotted the PCA scree plot and a bar chart of the top ten principal components’ explained variance, as shown in Figure~\ref{fig:pca-var}.  
Both plots reveal that the first three components contribute substantially more variance than the rest, with the third component explaining slightly less variance than the first two but significantly more than the fourth.  
This supports the observation that the 2D projection loses some discriminative information, which explains why the best ARI achieved by PCA remains moderate (0.3614).  

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{Results/Problem_1_a/pca_scree.png}
    \includegraphics[width=0.45\textwidth]{Results/Problem_1_a/pca_top10_var.png}
    \caption{(Left) Scree plot showing the variance explained by each principal component; (Right) bar chart of the top-10 PCs' explained variance ratios.}
    \label{fig:pca-var}
\end{figure}

Furthermore, I visualized the top ten PCA component images (Figure~\ref{fig:pca-components}), which illustrate the principal modes of variation across digits.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Results/Problem_1_a/pca_components.png}
    \caption{Top ten PCA components visualized as $8\times8$ basis images.}
    \label{fig:pca-components}
\end{figure}

\paragraph{NMF$\rightarrow$PCA.}
For the NMF method, I first determined the optimal number of components by minimizing the reconstruction error, which yielded $k=20$. 
This means the data were decomposed into 20 non-negative basis vectors. 
The resulting coefficient matrix $W$ was then compressed into a 2D embedding space using PCA for visualization and comparison purposes, and the final result is shown in Figure~\ref{fig:linear-2d-all}(b).

As observed in the plot, NMF fails to clearly separate the digits in the 2D space. 
This is consistent with the quantitative results, where NMF achieved the lowest ARI (0.1588) and NMI (0.2961) among all methods. 
These findings indicate that the NMF representation captures local, part-based features rather than global discriminative structures, and that the subsequent PCA compression may distort these part-based patterns, reducing the overall interpretability. 

On the other hand, NMF obtained the highest Silhouette score (0.4080), slightly higher than PCA. 
The embedding shows that nearly all samples are concentrated in the positive subspace, with only a small portion extending below zero (no less than $-0.2$). 
Even after PCA compression, this non-negativity-induced structure is largely preserved, resulting in an asymmetric distribution that nevertheless exhibits slightly better cluster compactness than PCA. 
This suggests that some of the features extracted by NMF may be better suited to local clustering in this dataset.
In particular, compared to PCA, NMF tends to focus on localized regions of variation rather than global variance directions, which likely contributes to the formation of tighter, more compact clusters.

Similar to the PCA case where excluding the third principal component led to a loss of explanatory power, it is plausible that the 2D projection of NMF also omits important structural information. 
Specifically, the visualization reveals that the first principal component successfully separates digit ``4'' along the left margin, while the second principal component distinguishes digits ``9'' (light blue) and ``0'' from the rest of the samples, forming a clear boundary near the bottom region of the plot. 
This indicates that the first PC primarily captures the unique pattern of the digit ``4'', whereas the second PC isolates the shapes shared by ``0'' and ``9''. 
A potential 3D embedding including an additional principal component might further clarify the remaining overlapping clusters visible in the upper-right portion of the 2D space.

Finally, I visualized the NMF basis components, as shown in Figure~\ref{fig:nmf-components}. 
These components represent localized stroke-like patterns, providing an interpretable decomposition of the digits into additive parts.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Results/Problem_1_a/nmf_components.png}
    \caption{Top ten PCA components visualized as $8\times8$ basis images.}
    \label{fig:nmf-components}
\end{figure}


%-------------------------------
% ICA
% ------------------------------
\paragraph{ICA.}
For the ICA method, I applied FastICA with two components after standardizing the data to ensure consistent feature scaling. 
As shown in Figure~\ref{fig:linear-2d-all}(c) and the summary table, ICA exhibits the most balanced overall performance among the three linear methods. 
Some clusters---notably digits 4, 6, and 0---are well separated and relatively compact, outperforming both PCA and NMF in local structure preservation. 
However, the remaining digits appear more mixed than in PCA, though less dispersed than in NMF. 
Visually, the embedding forms two major groups: one consisting primarily of digits 4, 6, and 0, and another containing the other seven digits.

Quantitatively, ICA’s ARI (0.3175) and NMI (0.4567) values lie between those of PCA and NMF, while its Silhouette score (0.3673) is the lowest among the three. 
This aligns with the visual interpretation: ICA achieves moderate global separability but weaker overall cluster compactness.

Regarding hyperparameters, ICA provides limited tuning options. Here I set \\
\texttt{components}=2, reducing the data to a 2D space, and standardized all features prior to decomposition. 
Standardization is a conventional preprocessing step for ICA, as its underlying assumption relies on statistical independence between components. 
Consequently, the resulting embedding appears nearly spherical and centered around the origin---a distribution consistent with ICA’s model assumptions but inconsistent with the inherent non-spherical structure of handwritten digits. 
This mismatch explains the weaker performance of ICA in this task.

Finally, the ICA component images (Figure~\ref{fig:ica-components}) reveal contrast-like and edge-detecting patterns that emphasize stroke boundaries, differing from the smoother, global variations captured by PCA and the localized parts extracted by NMF.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Results/Problem_1_a/ica_components.png}
    \caption{Top ten PCA components visualized as $8\times8$ basis images.}
    \label{fig:ica-components}
\end{figure}

%-------------------------------
% Conclusion
% ------------------------------
\paragraph{Overall Discussion.}
In summary, among the three linear dimension reduction techniques, PCA stands out as the most effective approach. 
It achieved the highest ARI (0.3614) and NMI (0.5190), and a Silhouette score (0.3993) close to the best. 
Visually, PCA produced the clearest and most interpretable clusters in the 2D embedding, separating several digits (such as 0, 3, 4, 6, and 9) distinctly. 
Given that PCA captures global variance directions, its performance would likely improve further in higher-dimensional embeddings, where additional components could better preserve discriminative variance. 

%-------------------------------
% Problem 1b
% ------------------------------
\subsection{Apply manifold learning approaches.}

















% -------------------------------
% Problem 2
% -------------------------------
\section{Open-Ended Data Analysis - Breast Cancer gene expression data.}
\subsection{Preprocessing}
After loading the data (shape $445\times359$), six clinical columns were identified: \texttt{subtype}, \texttt{er\_status}, \texttt{pr\_status}, \texttt{her2\_status}, \texttt{node}, and \texttt{metastasis}, leaving $353$ gene expression features. 
No missing values or zero-variance genes were found, indicating high data quality. 
Each gene feature was standardized via Z-score normalization. 
Descriptive statistics of the clinical variables are summarized below:

\begin{itemize}
    \item \textbf{Subtype:} Luminal~A (200), Luminal~B (106), Basal-like (79), HER2-enriched (53), Normal-like (7).
    \item \textbf{ER-Status:} Positive (339), Negative (100), Performed but Not Available (2), Indeterminate (2), Not Performed (2).
    \item \textbf{PR-Status:} Positive (291), Negative (147), Indeterminate (3), Performed but Not Available (2), Not Performed (2).
    \item \textbf{HER2-Status:} Negative (371), Positive (65), Equivocal (5), Not Available (4).
    \item \textbf{Node:} mean = 0.73, std = 0.87, range = [0, 3].
    \item \textbf{Metastasis:} mean = 0.025, std = 0.155 (mostly non-metastatic samples).
\end{itemize}

\subsection{Methodology}
Five dimension reduction methods were applied to the standardized gene expression matrix to obtain two-dimensional embeddings:
\begin{enumerate}
    \item \textbf{Principal Component Analysis (PCA)} --- linear orthogonal projection capturing maximal variance.
    \item \textbf{Non-negative Matrix Factorization (NMF)} --- parts-based representation using non-negative constraints (run on non-standardized data to ensure non-negativity).
    \item \textbf{Spectral Embedding} --- manifold learning based on graph Laplacian eigenvectors.
    \item \textbf{t-SNE} --- nonlinear embedding preserving local neighborhood structures.
    \item \textbf{UMAP} --- manifold approximation balancing local and global relationships.
\end{enumerate}

Each embedding was visualized by coloring the points according to all six available clinical variables.  
However, for brevity and visual clarity, only the results colored by \textbf{Subtype} are shown here as representative examples.  
This selection illustrates overall trends while the complete set of figures (for all six variables across all five methods) is provided in the supplementary materials.

To quantitatively evaluate clustering quality with respect to molecular subtypes, $k$-means clustering ($k=5$) was applied to each embedding, and three metrics were computed: Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and Silhouette coefficient.


\subsection{Results}

Overall, nonlinear manifold-based methods (t-SNE and UMAP) outperformed linear approaches in terms of ARI and NMI, suggesting that the underlying structure of gene expression data is highly nonlinear. 
UMAP achieved the highest Silhouette score (0.44), indicating that it produces well-separated clusters with clear boundaries, while t-SNE yielded the best ARI (0.21) and NMI (0.27), showing the strongest alignment with the known PAM50 subtypes. 
PCA achieved moderate performance, confirming its ability to preserve global variance but limited capacity to capture complex nonlinear relations. 
NMF performed comparably, producing slightly denser local clusters (reflected in its higher Silhouette score) but weaker subtype separation. 
Spectral embedding produced the most compact clusters but with limited biological interpretability due to its sensitivity to graph construction.

\begin{table}[h!]
\centering\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{ARI} & \textbf{NMI} & \textbf{Silhouette} \\
\midrule
PCA & 0.1857 & 0.2399 & 0.3311 \\
NMF & 0.1712 & 0.2536 & 0.3842 \\
Spectral & 0.1421 & 0.2300 & 0.4020 \\
t-SNE & \textbf{0.2074} & \textbf{0.2704} & 0.3984 \\
UMAP & 0.2028 & 0.2780 & \textbf{0.4376} \\
\bottomrule
\end{tabular}
\caption{Quantitative comparison of five dimension reduction methods on the BRCA dataset. Evaluation is based on clustering alignment with PAM50 subtypes ($k=5$).}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.32\textwidth]{Results/Problem_2/PCA_subtype.png}
    \includegraphics[width=0.32\textwidth]{Results/Problem_2/NMF_subtype.png}
    \includegraphics[width=0.32\textwidth]{Results/Problem_2/Spectral_subtype.png}
    \includegraphics[width=0.32\textwidth]{Results/Problem_2/tSNE_subtype.png}
    \includegraphics[width=0.32\textwidth]{Results/Problem_2/UMAP_subtype.png}
    \caption{Two-dimensional embeddings of the BRCA gene expression data using five methods (PCA, NMF, Spectral, t-SNE, and UMAP), colored by molecular Subtype. All embeddings were also generated for the remaining five clinical variables, but only Subtype-colored results are displayed here for clarity.}
\end{figure}

\subsection{Discussion}
Given that no missing or low-variance genes were present, preprocessing had minimal impact on the raw data distribution. 
The observed performance differences mainly stem from the intrinsic characteristics of each method. 
The superior performance of UMAP and t-SNE suggests that manifold learning effectively captures nonlinear relationships among gene expressions that correspond to known molecular subtypes.
Future work could extend this analysis by increasing latent dimensionality or incorporating autoencoders for deeper nonlinear representations.







\newpage
\appendix
\section{Appendix: Code Implementation}
\begin{python}

\end{python}


\end{document}
