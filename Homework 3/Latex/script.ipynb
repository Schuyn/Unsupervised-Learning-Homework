{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3b3a00",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1677780",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ab5b0",
   "metadata": {},
   "source": [
    "### (a)Download and process daily stock return data from January 2021 - present.\n",
    "For the following 15 companies: [”AAPL”,”MSFT”,”GOOGL”,”AMZN”,”META”,”NVDA”,”JPM”,”BAC”,”XOM”,”CVX”,”JNJ”,”PFE”,”WMT”,”PG”,”KO”] (苹果、微软、谷歌、亚马逊、Meta、英伟达、摩根大通、美国银行、埃克森美孚、雪佛龙、强生、辉瑞、沃尔玛、宝洁、可口可乐). You may want to apply a log or other transform to this data. Briefly visually explore this data before applying graphical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f9a9f",
   "metadata": {},
   "source": [
    "报告：我选择在读入数据后通过日收盘价计算各个公司的对数收益率，并在此数据的基础上进行之后的操作。\n",
    "\n",
    "在应用graphic model之前，我先对数据进行了一些viz exploration。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c91f56",
   "metadata": {},
   "source": [
    "描述性统计：\n",
    ",AAPL,AMZN,BAC,CVX,GOOGL,JNJ,JPM,KO,META,MSFT,NVDA,PFE,PG,WMT,XOM\n",
    "count,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0,1228.0\n",
    "mean,0.0006252817481255967,0.0002652867207352681,0.0005343148024555381,0.0006382533442478156,0.0010192321374135876,0.000325962485473575,0.0008075795932814771,0.0003806515088189639,0.0006506168797275214,0.0006640575245805652,0.002130151114063647,-0.00011513890752421391,0.0001758158115187743,0.000679399519558362,0.0010088768263657505\n",
    "std,0.017639507952617345,0.022275827804355667,0.01717759621689925,0.01603173841911638,0.019635913756156077,0.01053022697990614,0.015270281900666215,0.010033363619190095,0.02782667461515523,0.016263317781444485,0.03287767691562012,0.015876697804185495,0.010876250060137415,0.013221305293928353,0.017134748039508447\n",
    "min,-0.09701349313379018,-0.15139791287665294,-0.11724546514030931,-0.0858240889197669,-0.09992410100868955,-0.07895281436793397,-0.07778683460814495,-0.0721684421233099,-0.30639057815035836,-0.08029545476398928,-0.18594603665397563,-0.06954275150263382,-0.06434863391498345,-0.12076469377510593,-0.08213526323412572\n",
    "25%,-0.0080777836033277,-0.011357356689743867,-0.009057148636385872,-0.007773751459267388,-0.009514359692766905,-0.005226271768141855,-0.007294167564171912,-0.0052660861588273026,-0.011602496197326295,-0.007620294490314162,-0.016207447976086378,-0.009371748642833931,-0.005656291415177972,-0.005660837305979604,-0.008965598704313753\n",
    "50%,0.0010952492494840601,0.0004177776296910137,0.0003768928335541169,0.0013470380862022114,0.0016272266042002888,0.0004031589567810894,0.0012027094111862415,0.0006915877926995831,0.0007486747993638544,0.0007064731134312565,0.002900135874179596,-0.0005499421473920522,0.000611659319870032,0.0010000997107440864,0.0013377125818866055\n",
    "75%,0.009986694626128447,0.012504139306510353,0.010032443815564885,0.009490062473953243,0.011459865531069632,0.005902030846883108,0.009258257286066532,0.006028747927386033,0.0138330669045548,0.009926127944819393,0.02145622263635865,0.008488608621597827,0.006578798168987353,0.007656659656118281,0.0115014477692146\n",
    "max,0.14261752825727164,0.1269489157684572,0.08092385846601217,0.0852923426871832,0.09734782084196751,0.06008997499124939,0.1092537170528575,0.046167860304529185,0.20930762824058982,0.09652468154921455,0.2180878827528389,0.10305437736841412,0.04181267241721929,0.09120027209359872,0.06214127711947894\n",
    "\n",
    "可以看到芯片股(NVDA)收益最高（平均日收益为0.213%）但风险最大（波动3.29%为最大，回撤幅度18.6%最高），消费股(KO、PG)最稳定但收益低，体现了风险与收益的权衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d1afd8",
   "metadata": {},
   "source": [
    "在correlation matrix中，强相关对（0.6以上）：\n",
    "\n",
    "AMZN-AAPL(0.56)、MSFT-AAPL(0.64)、MSFT-AMZN(0.66)：科技巨头高度同步，往往一起涨跌\n",
    "MSFT-GOOGL(0.65)、MSFT-NVDA(0.64)：科技板块内部联动紧密\n",
    "JPM-BAC(0.82)：金融股强相关，受经济周期影响一致\n",
    "CVX-XOM(0.86)：能源股高度相关，都受油价驱动\n",
    "\n",
    "中等相关（0.4-0.6）：\n",
    "\n",
    "金融与科技间：JPM与科技股多在0.3-0.4之间\n",
    "消费与科技：WMT、PG与科技股多在0.2-0.4\n",
    "\n",
    "低相关或弱相关（<0.3）：\n",
    "\n",
    "JNJ(制药)与大多数股票相关性低（0.02-0.18），抗周期性强\n",
    "PFE(制药)与其他股票多在0.1-0.3，医药股独立性好\n",
    "XOM(能源)与科技股多在0.1-0.2，反映行业差异\n",
    "\n",
    "负相关：强生和英伟达是负相关对（-0.09），展示了比较有趣的相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff651d0",
   "metadata": {},
   "source": [
    "除此以外，我还绘制了各公司的累计回报和行业或板块内部的回报相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374afce3",
   "metadata": {},
   "source": [
    "### (b) Fit undirected graphical models via the Graphical Lasso and a non-parametric version of the Graphical Lasso. \n",
    "\n",
    "You should use proper hyperparameter tuning to determine the sparsity of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d207f97",
   "metadata": {},
   "source": [
    "最终结果如图（Homework 3\\Latex\\Figures\\1b_glasso_precision_matrices.png）所示，可以看到两张图几乎完全一致，这说明股票数据股票收益虽然 heavy-tailed，但在依赖结构上已经很接近 Gaussian copula 了，因此 Spearman-based nonparanormal GL 与 Gaussian GL 结果高度一致，由此我可以进行推断：这个数据的条件依赖结构稳定、低维且主要由行业因子驱动。   \n",
    "\n",
    "从结果上看，Amazon和可口可乐之间形成了极强的条件相关性，类似的还有JPM和Google，沃尔玛和美国银行，英伟达和PFE等，也都有较为明显的条件相关关系，说明在固定了其他因子的条件下，这些组合的收益率会受到彼此的显著影响。从聚类形态上看，apple，msft，Google，Amazon和meta形成了一个正相关的簇（图中下三角的上半部），虽然从事实上讲nvidia应该也是其中之一，但是在图中并没有显示出明显的相关性，结合累计回报曲线看，这可能是由于nvidia在这段时间的异常表现（一骑绝尘，而且波动很大）所致。另外，在这张图中还能看到比较稀奇的nvidia以及bac和jnj的负相关关系，好似是jnj越好，这俩家的收益越糟。我认为这并不是由于任何市场因素，应该是2025年初的事件导致。\n",
    "\n",
    "我们应用了从log10(0.01)到log10(0.8)共30个点的grid search，通过cross-validated Gaussian log-likelihood 选出最优 α。最终选择了：\n",
    "Standard GL Best Alpha: 0.021287\n",
    "Non-Parametric GL Best Alpha: 0.013528\n",
    "由于篇幅受限，所以仅展示standard glasso的调参过程（Homework 3\\Latex\\Figures\\1b_glasso_cv_curve.png）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b027d",
   "metadata": {},
   "source": [
    "### (c) Fit a directed graphical model via the PC algorithm or a structural equation model. \n",
    "\n",
    "You should again tune any hyperparameters to determine graph sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f4b58",
   "metadata": {},
   "source": [
    "我通过计算每张图的bic score来衡量模型在alphas=[0.001, 0.01, 0.05, 0.1, 0.2]的表现，默认最好的alpha依旧是约定俗成的5%，最终的结果如图（Figures\\1c_pc_tuning.png）所示模型在alpha为0.1的时候达到了最低的bic score，因此选择alpha=0.1进行建图，最终结果如图（Figures\\1c_pc_graph.png）所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea6aa1",
   "metadata": {},
   "source": [
    "### (d) Compare and interpret your graphical model estimates. Do the graphs relate companies as you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc3b97",
   "metadata": {},
   "source": [
    "从c的结果中可以看到科技股之间的依赖关系要明显多于其余板块，比如微软，应该是所有其余科技股的子结点，而NVIDIA与谷歌除了彼此之间的无向边之外是其余所有连向他们的结点的父亲结点，这说明科技股之间是有着显著、方向明确的依赖关系的，这点与b的结论相同。其余的公司则展现的是相对孤立的状态，比如美孚，仅仅与同板块的雪佛兰有一条依赖关系，这说明在给定雪佛兰的情况下，美孚甚至是与整个池中的其余个股完全独立的，突出了能源板块之间的强关联以及与其余板块之间“置身事外”的孤立性。\n",
    "\n",
    "比较有趣的地方在于b的结果中展现出显著依赖关系的KO-AMZN和JPM-GOOGL，在c的结果中却都没有展示出比较直接的联系，展示出有向图构建与glasso的逻辑区别。但是其中的联系也并不一定弱，比如KO可以通过苹果或者沃尔玛两个方式与Amazon构建联系，这点似乎也符合事实的直觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f1e88",
   "metadata": {},
   "source": [
    "### (e) Fit a Granger causal graphical model or a Vector Autoregressive Model (order 1 is fine) to this same data. Properly tune any hyperparameters. \n",
    "Does this order 1 model fit the data well? Interpret your results and compare these with your previous graph estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d4992",
   "metadata": {},
   "source": [
    "--- Granger Causality Test Results ---\n",
    "Significance level: 0.05\n",
    "Number of significant Granger causal relationships: 52\n",
    "Graph density: 0.248\n",
    "\n",
    "Top 10 strongest Granger causal relationships:\n",
    "Cause Effect  P-value  Lag\n",
    " META   AMZN 0.000002    1\n",
    " AAPL    JPM 0.000089    4\n",
    "  WMT     PG 0.000165    3\n",
    " AAPL   META 0.000319    4\n",
    "  WMT     KO 0.000415    1\n",
    " MSFT     PG 0.000963    4\n",
    " AAPL   MSFT 0.001364    3\n",
    "  BAC   AAPL 0.001686    2\n",
    " AAPL   AMZN 0.002355    4\n",
    " AAPL    BAC 0.002579    3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1ee94",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5290a",
   "metadata": {},
   "source": [
    "### (a) Apply kernel density estimation to this data and use this to generate new samples.\n",
    "You may apply kernel density estimation in a different space than the original data. You should also tune the bandwidth parameter. Are the samples generated from this density estimator recognizable as digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c72859",
   "metadata": {},
   "source": [
    "Dataset shape: (1797, 64)\n",
    "Number of classes: 10\n",
    "Pixel value range: [0.0, 16.0]\n",
    "Mean pixel value: 4.88 ± 6.02\n",
    "Hyperparameter Tuning: Bandwidth Selection\n",
    "  Best bandwidth: 0.579639\n",
    "  Cross-validation score: -8081.7343\n",
    "  Best bandwidth: 0.579639\n",
    "  Cross-validation score: -22008.9223\n",
    "  Best bandwidth: 0.454878\n",
    "  Cross-validation score: -19606.1587"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd1e64",
   "metadata": {},
   "source": [
    "Mean (Original):     4.8842\n",
    "Mean (Generated):    4.9478\n",
    "Std  (Original):     6.0168\n",
    "Std  (Generated):    5.6388\n",
    "Sparsity (Original): 0.4893\n",
    "Sparsity (Generated):0.2391\n",
    "KNN Accuracy (Original): 0.9627"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375abf98",
   "metadata": {},
   "source": [
    "对于kde而言唯一重要的参数是band width h，所以我在这部分实验中对bandwith进行了grid search，对每个带宽应用5折cv进行验证kde.score(X_test)并返回平均对数似然，取其中最大的。图Figures\\2a_kde_tuning.png展示的是kde模型分别在应用了pca降维到20、50以及原始空间上的不同最优带宽。由于kde在直接应用到原始空间时往往会出现维度灾难，因此我必须先将其降至低维后再进行kde采样，在后面的实验中我均选择采用pca20的模型，因此其余两个空间下的结果仅作展示用。\n",
    "\n",
    "图Figures\\2a_kde_tuning.png展示了生成的数据，可以看到一部分数字是颇有辨识度的，比如5 8 0 1 4等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444b801",
   "metadata": {},
   "source": [
    "对带宽参数h进行了网格搜索，在三个空间（PCA-20/50和原始64维）上分别使用5折交叉验证选择最优值，以平均对数似然为评价标准。由于原始空间的CV得分显著恶化（-19606 vs -8081），验证了维度灾难的影响，因此选择PCA-20空间（最优h=0.580）进行后续采样。生成样本的像素强度分布与原始数据高度匹配（均值4.88 vs 4.95，标准差6.02 vs 5.64），但空间结构存在差异：生成样本的稀疏度仅为23.91%，相比原始的48.93%低50%，表明KDE倾向于生成更密集的像素分布，导致一些数字（1、0、8等）清晰可辨，但整体对比度不足。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61165374",
   "metadata": {},
   "source": [
    "### (b) Build a Variational Autoencoder (VAE) OR a Generative Adversarial Network (GAN) to generate new samples.\n",
    "Rigorously tune any hyperparameters.Are the samples generated recognizable as digits? Are all possible digits generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7a47a",
   "metadata": {},
   "source": [
    "  Mean Diff: 0.0355 | Std Diff: 0.0695 | Diversity: 1.799\n",
    "  Mean Diff: 0.0399 | Std Diff: 0.0146 | Diversity: 1.615\n",
    "  Mean Diff: 0.0244 | Std Diff: 0.0081 | Diversity: 1.693\n",
    "  Mean Diff: 0.0394 | Std Diff: 0.0244 | Diversity: 1.505\n",
    "  Mean Diff: 0.0234 | Std Diff: 0.0772 | Diversity: 1.507\n",
    "  Mean Diff: 0.0610 | Std Diff: 0.0108 | Diversity: 1.386\n",
    "  Mean Diff: 0.0665 | Std Diff: 0.0384 | Diversity: 1.821\n",
    "  Mean Diff: 0.0521 | Std Diff: 0.0669 | Diversity: 1.648\n",
    "  Mean Diff: 0.0530 | Std Diff: 0.0693 | Diversity: 1.622\n",
    "  Mean Diff: 0.0614 | Std Diff: 0.0428 | Diversity: 1.527\n",
    "  Mean Diff: 0.1073 | Std Diff: 0.0362 | Diversity: 1.516\n",
    "  Mean Diff: 0.0274 | Std Diff: 0.0759 | Diversity: 1.422\n",
    "  Mean Diff: 0.0495 | Std Diff: 0.0619 | Diversity: 1.809\n",
    "  Mean Diff: 0.0427 | Std Diff: 0.0224 | Diversity: 1.695\n",
    "  Mean Diff: 0.0303 | Std Diff: 0.0810 | Diversity: 1.683\n",
    "  Mean Diff: 0.0049 | Std Diff: 0.0806 | Diversity: 1.558\n",
    "  Mean Diff: 0.0008 | Std Diff: 0.0301 | Diversity: 1.569\n",
    "  Mean Diff: 0.0576 | Std Diff: 0.1575 | Diversity: 1.434\n",
    "Tuning Results Summary:\n",
    " latent_dim    lr_g   g_hidden   d_hidden  mean_diff  std_diff  diversity_ratio\n",
    "         32 0.00005 [128, 256] [256, 128]   0.035519  0.069522         1.798594\n",
    "         32 0.00005 [256, 512] [512, 256]   0.039905  0.014617         1.614807\n",
    "         32 0.00010 [128, 256] [256, 128]   0.024352  0.008129         1.692949\n",
    "         32 0.00010 [256, 512] [512, 256]   0.039375  0.024415         1.504985\n",
    "         32 0.00020 [128, 256] [256, 128]   0.023443  0.077174         1.507142\n",
    "         32 0.00020 [256, 512] [512, 256]   0.061023  0.010811         1.385984\n",
    "         64 0.00005 [128, 256] [256, 128]   0.066535  0.038420         1.820829\n",
    "         64 0.00005 [256, 512] [512, 256]   0.052085  0.066898         1.648213\n",
    "         64 0.00010 [128, 256] [256, 128]   0.053011  0.069283         1.621573\n",
    "         64 0.00010 [256, 512] [512, 256]   0.061386  0.042757         1.526785\n",
    "         64 0.00020 [128, 256] [256, 128]   0.107317  0.036166         1.515745\n",
    "         64 0.00020 [256, 512] [512, 256]   0.027371  0.075890         1.421589\n",
    "        128 0.00005 [128, 256] [256, 128]   0.049508  0.061871         1.809039\n",
    "        128 0.00005 [256, 512] [512, 256]   0.042693  0.022421         1.695204\n",
    "        128 0.00010 [128, 256] [256, 128]   0.030315  0.081037         1.683092\n",
    "        128 0.00010 [256, 512] [512, 256]   0.004886  0.080638         1.558201\n",
    "        128 0.00020 [128, 256] [256, 128]   0.000816  0.030138         1.568593\n",
    "        128 0.00020 [256, 512] [512, 256]   0.057632  0.157499         1.434154"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a870a04",
   "metadata": {},
   "source": [
    "综合看来，latent_dim=32, lr_g=0.0001, G=[128, 256] 是最合适的参数组合：其 std_diff 最低（生成样本的分布形状最接近原始数据），mean_diff 也处于前列，且 latent=32 更轻量、不易过拟合，diversity ≈ 1.7 说明没有严重的 mode collapse。相比之下，latent=128 的组合虽然 mean_diff 更低，但 std_diff 差距明显，说明生成分布的\"形状\"偏离较大。\n",
    "其余超参数如 αD\\alpha_D\n",
    "αD​ 固定为 0.0002，与 DCGAN 论文一致。最终选择 αG<αD\\alpha_G < \\alpha_D\n",
    "αG​<αD​，即让 Generator 学习更慢，因为在之前 αG=αD\\alpha_G = \\alpha_D\n",
    "αG​=αD​ 的实验中，Discriminator 被 Generator 主导导致训练崩溃。\n",
    "\n",
    "在最佳参数下，模型训练过程如图Figures\\2b_gan_training.png所示。与之前的实验相比，两者的 loss 相对收敛，基本稳定在 1.0–1.2，Discriminator 准确率也超过 0.5。但分辨真实样本与生成样本的准确率存在显著差异：分辨假样本时准确率稳定在 0.8 以上，而分辨真样本时却不足 0.6。这反映了 Vanilla GAN 的固有缺陷——min-max 博弈难以达到纳什均衡，也解释了后续 WGAN、Diffusion Models 等方法的研究动机。\n",
    "生成样本示例如图Figures\\2b_generated_samples_gan.png所示，部分数字（如 0、1、2、3、5、6、9）具有较高辨识度，整体表现明显优于 KDE。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0226a",
   "metadata": {},
   "source": [
    "### (c) Apply a denoising diffusion model to generate new samples from this data. \n",
    "Comapre the samples generated to those from the above methods and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2c55d",
   "metadata": {},
   "source": [
    "Tuning Results Summary:\n",
    " num_timesteps     lr     hidden_dims  final_loss  mean_diff  std_diff  diversity_ratio\n",
    "           200 0.0001 [128, 256, 128]    0.397042   0.780336  0.308475         2.122012\n",
    "           200 0.0001 [256, 512, 256]    0.306661   0.218310  0.445004         1.683569\n",
    "           200 0.0005 [128, 256, 128]    0.278427   0.169971  0.539205         1.640831\n",
    "           200 0.0005 [256, 512, 256]    0.240664   0.020464  0.744339         1.451204\n",
    "           200 0.0010 [128, 256, 128]    0.258257   0.089139  0.645056         1.549303\n",
    "           200 0.0010 [256, 512, 256]    0.233321   0.007569  0.719233         1.437204\n",
    "           500 0.0001 [128, 256, 128]    0.346644   1.303562  1.322603         2.274626\n",
    "           500 0.0001 [256, 512, 256]    0.240584   0.483363  0.561203         1.992378\n",
    "           500 0.0005 [128, 256, 128]    0.212621   0.343480  0.113067         1.814647\n",
    "           500 0.0005 [256, 512, 256]    0.176663   0.005402  0.567972         1.479918\n",
    "           500 0.0010 [128, 256, 128]    0.194704   0.100521  0.530461         1.578311\n",
    "           500 0.0010 [256, 512, 256]    0.168762   0.012980  0.660851         1.452953\n",
    "          1000 0.0001 [128, 256, 128]    0.320002   2.692240  1.878439         2.355001\n",
    "          1000 0.0001 [256, 512, 256]    0.205096   2.010370  1.639684         2.241753\n",
    "          1000 0.0005 [128, 256, 128]    0.173627   1.267177  1.130676         2.109792\n",
    "          1000 0.0005 [256, 512, 256]    0.141317   0.133437  0.432766         1.576990\n",
    "          1000 0.0010 [128, 256, 128]    0.150619   0.225414  0.272470         1.643631\n",
    "          1000 0.0010 [256, 512, 256]    0.132122   0.054404  0.559425         1.483325\n",
    "Best Configuration (by Mean Diff):\n",
    "  Timesteps: 500\n",
    "  Learning Rate: 0.0005\n",
    "  Hidden Dims: [256, 512, 256]\n",
    "  Mean Diff: 0.0054\n",
    "  Std Diff: 0.5680"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e3fc8",
   "metadata": {},
   "source": [
    "本次实验中，我选择优化 lr、timesteps 和 architecture 三个参数。在所有 18 种组合中，通过 mean_diff 和 std_diff 的热力图来确定 lr 和 timesteps，并用 final loss 来确定 architecture。结果如图 X 所示。\n",
    "从 final loss 来看，architecture [256, 512, 256] 的平均表现明显优于 [128, 256, 128]。在此基础上，timesteps=500、lr=0.0005 的组合取得了最低的 mean_diff（0.0054），std_diff 也处于可接受范围。因此，最终选择 lr=0.0005, timesteps=500, architecture=[256, 512, 256] 作为最优模型设定，后续讨论均基于此配置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aafa4d",
   "metadata": {},
   "source": [
    "Training Diffusion Model...\n",
    "  Timesteps: 500\n",
    "  Hidden dims: [256, 512, 256]\n",
    "  Learning rate: 0.0005\n",
    "  Epochs: 200, Batch size: 128\n",
    "Epoch [  20/200] | Loss: 0.302732\n",
    "Epoch [  40/200] | Loss: 0.249733\n",
    "Epoch [  60/200] | Loss: 0.230896\n",
    "Epoch [  80/200] | Loss: 0.214719\n",
    "Epoch [ 100/200] | Loss: 0.193410\n",
    "Epoch [ 120/200] | Loss: 0.185098\n",
    "Epoch [ 140/200] | Loss: 0.182768\n",
    "Epoch [ 160/200] | Loss: 0.172739\n",
    "Epoch [ 180/200] | Loss: 0.191683\n",
    "Epoch [ 200/200] | Loss: 0.165001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e72fe",
   "metadata": {},
   "source": [
    "模型的训练过程如图 Figures\\2c_diffusion_training.png 所示，生成结果如图 Figures\\2c_generated_samples_diffusion.png 所示。对比三个模型的生成结果，可得出如下结论：\n",
    "从统计特征来看，GAN 生成的样本亮度更高、对比度更强，Sparsity 也更接近原始数据，说明生成器较好地学习到了数据的整体分布特征。然而从结构清晰度来看，Diffusion 生成的样本轮廓更清晰、数字结构更易辨识，而 KDE 和 GAN 生成的样本则更加模糊。\n",
    "GAN 在结构清晰度上表现不佳的原因在于判别器过弱——无法准确分辨真实与生成样本。因此，尽管生成器学习到了原始数据的统计特征，但由于能轻易\"骗过\"判别器，缺乏进一步提升真实性的动力。而 Vanilla GAN 对超参数高度敏感，难以找到使 G/D 达到理想平衡的配置，这正是其与 Diffusion 产生差距的根本原因。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
