\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{pythonhighlight}
\usepackage{subcaption} % 并列放图时可以分别打标签


% 页边距
\geometry{margin=1in}

% 页眉页脚
\pagestyle{fancy}
\fancyhf{}
\lhead{STAT 5244 -- Unsupervised Learning}
\rhead{HW2}
\cfoot{\thepage\ of \pageref{LastPage}}

% -------------------------------
% 数学环境
% -------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

% 向量/矩阵粗体
\renewcommand{\vec}[1]{\bm{#1}}

% -------------------------------
% 代码高亮
% -------------------------------
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  frame=single,
  breaklines=true,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  tabsize=4
}

% -------------------------------
% 文档开始
% -------------------------------
\begin{document}

\begin{center}
    {\Large \textbf{STAT 5244 -- Unsupervised Learning}}\\[6pt]
    \textbf{Homework 2}\\[6pt]
    Name: \underline{Chuyang Su} \quad UNI: \underline{cs4570}
\end{center}

\hrule
\vspace{1em}

% -------------------------------
% Problem 1
% -------------------------------
\section{Mixture Models}
\subsection{EM Algorithm Derivation}
Since we model count-valued data, assume each observation $x_i=(x_{i1},\ldots,x_{ip})\in\mathbb{N}_0^p$ is generated from a finite mixture of \emph{independent} Poisson distributions:
\[
p(x_i;\,\pi,\lambda)
=\sum_{k=1}^K \pi_k\, \prod_{j=1}^p \frac{e^{-\lambda_{kj}}\lambda_{kj}^{x_{ij}}}{x_{ij}!},
\quad \pi_k\ge 0,\ \sum_{k=1}^K \pi_k=1,\ \lambda_{kj}>0.
\]
Here $\pi=(\pi_1,\ldots,\pi_K)$ are mixture weights and $\lambda_k=(\lambda_{k1},\ldots,\lambda_{kp})$ are component-wise Poisson means.

\paragraph{Latent variables.}
Introduce latent indicators $z_{ik}\in\{0,1\}$ with $\sum_{k=1}^K z_{ik}=1$, where $z_{ik}=1$ if $x_i$ comes from component $k$. The complete-data likelihood is
\[
L_c(\pi,\lambda)=\prod_{i=1}^n\prod_{k=1}^K
\Bigg[
\pi_k \prod_{j=1}^p \frac{e^{-\lambda_{kj}}\lambda_{kj}^{x_{ij}}}{x_{ij}!}
\Bigg]^{z_{ik}}.
\]
Taking logs and dropping constants independent of $(\pi,\lambda)$ (i.e., $\log x_{ij}!$) gives the complete-data log-likelihood
\[
\ell_c(\pi,\lambda)
\varpropto \sum_{i=1}^n \sum_{k=1}^K z_{ik}
\left[
\log \pi_k+\sum_{j=1}^p \big(x_{ij}\log \lambda_{kj}-\lambda_{kj}\big)
\right].
\]

\paragraph{E-step Derivation.}
Since the latent indicators $z_{ik}$ are unobserved, we take their conditional expectation under the current parameters.
Define
\[
\gamma_{ik} := \mathbb{E}[z_{ik} \mid x_i;\pi^{(t)},\lambda^{(t)}]
= P(z_{ik}=1 \mid x_i;\pi^{(t)},\lambda^{(t)}),
\]
which represents the posterior probability that observation $x_i$ belongs to component $k$.

Using Bayes' theorem,
\[
P(z_{ik}=1 \mid x_i;\pi^{(t)},\lambda^{(t)})
=\frac{P(z_{ik}=1;\pi^{(t)})\,P(x_i\mid z_{ik}=1;\lambda^{(t)})}
{P(x_i;\pi^{(t)},\lambda^{(t)})}.
\]
Each term can be expressed as:
\[
P(z_{ik}=1;\pi^{(t)}) = \pi_k^{(t)}, 
\quad
P(x_i\mid z_{ik}=1;\lambda^{(t)}) 
= \prod_{j=1}^p \frac{e^{-\lambda_{kj}^{(t)}} (\lambda_{kj}^{(t)})^{x_{ij}}}{x_{ij}!},
\]
and
\[
P(x_i;\pi^{(t)},\lambda^{(t)}) 
= \sum_{\ell=1}^K \pi_\ell^{(t)} 
\prod_{j=1}^p \frac{e^{-\lambda_{\ell j}^{(t)}} (\lambda_{\ell j}^{(t)})^{x_{ij}}}{x_{ij}!}.
\]

Substituting these expressions into Bayes’ rule yields:
\[
\gamma_{ik}
= \frac{
\pi_k^{(t)} \prod_{j=1}^p e^{-\lambda_{kj}^{(t)}} (\lambda_{kj}^{(t)})^{x_{ij}} / x_{ij}!}
{\sum_{\ell=1}^K \pi_\ell^{(t)} 
\prod_{j=1}^p e^{-\lambda_{\ell j}^{(t)}} (\lambda_{\ell j}^{(t)})^{x_{ij}} / x_{ij}!}.
\]

Since the term $\prod_{j=1}^p x_{ij}!$ does not depend on $k$, it cancels out between numerator and denominator. 
Therefore, the final expression for the responsibilities is:
\[
\boxed{
\gamma_{ik}
=
\frac{
\pi_k^{(t)} \prod_{j=1}^p e^{-\lambda_{kj}^{(t)}} (\lambda_{kj}^{(t)})^{x_{ij}}
}{
\sum_{\ell=1}^K \pi_\ell^{(t)} \prod_{j=1}^p e^{-\lambda_{\ell j}^{(t)}} (\lambda_{\ell j}^{(t)})^{x_{ij}}
}},
\quad i=1,\ldots,n,\quad k=1,\ldots,K.
\]

\paragraph{M-step.}
We maximize
\[
Q(\pi,\lambda)=
\sum_{i=1}^n\sum_{k=1}^K \gamma_{ik}
\left[
\log \pi_k+\sum_{j=1}^p \big(x_{ij}\log \lambda_{kj}-\lambda_{kj}\big)
\right]
\]
subject to $\pi_k\ge 0$, $\sum_{k=1}^K\pi_k=1$, and $\lambda_{kj}>0$.

\medskip
\emph{Update for $\pi_k$.}
Introduce a Lagrange multiplier $\eta$ for the simplex constraint:
\[
\mathcal{L}(\pi,\eta)
=\sum_{k=1}^K\Big(\sum_{i=1}^n \gamma_{ik}\Big)\log \pi_k
+\eta\!\left(1-\sum_{k=1}^K \pi_k\right).
\]
Setting the partial derivatives to zero,
\[
\frac{\partial \mathcal{L}}{\partial \pi_k}
=\frac{\sum_{i=1}^n\gamma_{ik}}{\pi_k}-\eta=0
\quad\Longrightarrow\quad
\pi_k=\frac{\sum_{i=1}^n\gamma_{ik}}{\eta}.
\]
Summing over $k$ and using $\sum_{k=1}^K\pi_k=1$ gives
\[
1=\sum_{k=1}^K \pi_k
=\frac{1}{\eta}\sum_{k=1}^K\sum_{i=1}^n\gamma_{ik}
=\frac{1}{\eta}\sum_{i=1}^n\sum_{k=1}^K\gamma_{ik}
=\frac{1}{\eta}\sum_{i=1}^n 1
=\frac{n}{\eta}
\;\Longrightarrow\;
\eta=n.
\]
Hence
\[
\boxed{\ \pi_k^{(t+1)}=\frac{1}{n}\sum_{i=1}^n \gamma_{ik}\ }.
\]
(Concavity: $\partial^2 \mathcal{L}/\partial \pi_k^2 = -(\sum_i\gamma_{ik})/\pi_k^2<0$.)

\medskip
\emph{Update for $\lambda_{kj}$.}
For each $(k,j)$, the terms of $Q$ that involve $\lambda_{kj}$ are
\[
Q_{kj}(\lambda_{kj})
=\sum_{i=1}^n \gamma_{ik}\big(x_{ij}\log\lambda_{kj}-\lambda_{kj}\big).
\]
Differentiate and set to zero:
\[
\frac{\partial Q_{kj}}{\partial \lambda_{kj}}
=\sum_{i=1}^n \gamma_{ik}\left(\frac{x_{ij}}{\lambda_{kj}}-1\right)=0
\quad\Longrightarrow\quad
\lambda_{kj}
=\frac{\sum_{i=1}^n \gamma_{ik}x_{ij}}{\sum_{i=1}^n \gamma_{ik}}.
\]
(Concavity: $\partial^2 Q_{kj}/\partial \lambda_{kj}^2
=-\sum_i \gamma_{ik}x_{ij}/\lambda_{kj}^2<0$ when $\lambda_{kj}>0$.) Therefore
\[
\boxed{\ \lambda_{kj}^{(t+1)}=\frac{\sum_{i=1}^n \gamma_{ik} x_{ij}}{\sum_{i=1}^n \gamma_{ik}}\ }.
\]

\subsection{ Interpretation and Comparison of Poisson and Gaussian Mixture Models.}

Both the Poisson Mixture Model (PMM) and the Gaussian Mixture Model (GMM) successfully converged and achieved almost identical clustering performance on the
author--chapter word-frequency data.


The PMM converged in 16 iterations with a clustering purity of \textbf{0.8989},
while the GMM converged in 14 iterations with a purity of \textbf{0.9001}.
The learned author--cluster mappings were consistent across models:
Cluster~0 corresponds to \emph{Shakespeare}, Cluster~1 to \emph{London}, and Clusters~2--3 to \emph{Austen},
indicating that Austen's writing exhibits two stylistically distinct sub-modes.

Examining the estimated centroids revealed interpretable linguistic patterns.
The Shakespeare cluster assigns high weights to archaic forms such as
\emph{thou}, \emph{thy}, and \emph{hath},
capturing the syntax of Early Modern English.
London's centroid emphasizes neutral verbs like \emph{was}, \emph{had},
and \emph{were}, representing narrative realism,
while Austen's two clusters differ primarily in pronoun and modal-verb usage:
one dominated by \emph{she, her, would, could} (social dialogue tone),
and the other by \emph{my, our, only, such} (introspective narration).

By inspecting the soft responsibilities $\gamma_{ik}$,
chapters with $\max_k \gamma_{ik} < 0.6$ were identified as
\emph{low-certainty chapters}.
These typically occur near stylistic transitions
or among authors with overlapping vocabularies.
Such ambiguous sections highlight that soft clustering provides richer insights
than hard assignments.

Overall, both mixture models achieve roughly 90\% purity and uncover meaningful
stylistic structures.
While the GMM yields a marginally higher purity,
the Poisson mixture remains theoretically more appropriate
for discrete count data and offers comparable empirical performance.

% -------------------------------
% Problem 2
% -------------------------------
\section{Open-Ended Cluster Analysis - Breast Cancer gene expression data.}
\subsection{Apply clustering techniques to explore this data set.}

To explore the internal structure of the BRCA gene-expression dataset, I first applied a two-step dimensionality reduction approach using \textbf{PCA} followed by \textbf{UMAP}. 

Based on conclusions drawn in Homework 1, this strategy provides an effective balance between global and local structure preservation: 
PCA removes redundant noise variables while retaining the main variance directions, and UMAP further compresses the PCA-transformed space, producing a low-dimensional embedding that reveals local neighborhood structure and facilitates visual interpretation. 
Following the empirical results from Homework 1, I selected 30 principal components (the ``elbow'' point in the cumulative variance curve) for PCA, 
and adopted UMAP parameters $n\_neighbors=10$, $min\_dist=0.0$, and $n\_components=2$ or $10$, where the two-dimensional embedding was used for visualization and the ten-dimensional representation served as the feature space for clustering.
The results are shown below:
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{"Figures/pam50__pca2.png"}
        \caption{PAM50 subtypes visualized in PCA(2D) space.}
        \label{fig:pam50_pca2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{"Figures/pam50__umap2.png"}
        \caption{PAM50 subtypes visualized in UMAP(2D) space.}
        \label{fig:pam50_umap2}
    \end{subfigure}
    \label{fig:pam50_embeddings}
\end{figure}

Subsequent clustering was performed using seven algorithms: \textbf{KMeans}, \textbf{Gaussian Mixture Model (GMM)}, \textbf{Spectral Clustering}, \textbf{DBSCAN}, and \textbf{Agglomerative Clustering} with three linkages (Ward, Average, and Complete). 
Given that the biological subtype reference (PAM50) contains five classes, all parametric clustering methods were set to $K=5$, while DBSCAN automatically determined the number of clusters.

Among these, \textbf{Spectral Clustering} was treated as a special case. 
Because it internally constructs a similarity graph and performs Laplacian eigen-decomposition—conceptually similar to UMAP’s graph-based embedding—it is inappropriate to apply Spectral Clustering directly in UMAP space. 
Doing so would perform a second spectral decomposition on an already nonlinear manifold, likely distorting the intrinsic geometry. 
Therefore, Spectral Clustering was conducted on the PCA(30D) features instead, ensuring consistency with its theoretical formulation.

For the Agglomerative Clustering, I compared only the \textbf{Ward}, \textbf{Average}, and \textbf{Complete} linkages, excluding \textbf{Single linkage} for three reasons: 
(1) it is highly sensitive to outliers and prone to chaining effects, which are amplified in high-dimensional continuous data; 
(2) its computational cost is high while its silhouette scores are unstable; and 
(3) the remaining three linkages already represent the major clustering behaviors—variance minimization (Ward), mean-distance smoothing (Average), and maximal-separation enforcement (Complete).

\vspace{0.5em}
\noindent\textbf{Insights from the clustering results.}
Rather than directly comparing algorithmic performance, the focus here is on what each clustering method reveals about the \emph{data manifold itself}. 
Across methods, two key structural patterns emerge. 
First, the dataset contains both compact and diffuse regions: \textbf{Basal-like} and \textbf{Normal-like} samples consistently form dense, isolated clusters, 
while \textbf{Luminal~A} and \textbf{Luminal~B} exhibit gradual overlap, suggesting a continuous transition rather than distinct boundaries. 
Second, the overall data distribution is non-spherical and heterogeneous in density, which explains why algorithms assuming isotropic clusters (KMeans, GMM, Ward) perform similarly and fail to separate overlapping subtypes. 

\textbf{DBSCAN} highlights these density variations most clearly—it detects three main groups and labels the remaining sparse regions as noise. 
Although this underestimates the total number of subtypes, it accurately reflects the intrinsic density imbalance of the dataset: the rarest subtypes (\textbf{HER2-enriched}, \textbf{Normal-like}) are naturally absorbed as low-density regions. 
\textbf{Average linkage} produces smoother transitions that capture the gradual relationship between Luminal subtypes, while \textbf{Complete linkage} emphasizes inter-cluster separation. 
Taken together, these clustering outcomes indicate that the BRCA expression data contain both discrete and continuous subtype structures—Basal-like being compact and distinct, 
whereas Luminal~A/B lie on a continuum of transcriptional profiles—consistent with known biological heterogeneity among breast cancer subtypes.



% -------------------------------
% Code
% -------------------------------
\newpage
\appendix
\section{Appendix: Code Implementation}


\end{document}